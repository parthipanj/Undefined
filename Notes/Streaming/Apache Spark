https://www.edureka.co/blog/spark-architecture/
https://medium.com/@josemarcialportilla/installing-scala-and-spark-on-ubuntu-5665ee4b62b1
https://spark.apache.org/downloads.html
https://medium.com/devilsadvocatediwakar/installing-apache-spark-on-ubuntu-8796bfdd0861
https://datawookie.netlify.com/blog/2017/07/installing-spark-on-ubuntu/

https://github.com/beeva/beeva-best-practices/blob/master/big_data/spark/README.md#tuning-and-debugging
https://www.bi4all.pt/en/news/en-blog/apache-spark-best-practices/

Apache Spark is an open source cluster computing framework for real-time data processing.

In-memory computing means using a type of middleware software that allows one to store data in RAM, across a cluster of computers, and process it in parallel. Consider operational datasets typically stored in a centralized database which you can now store in “connected” RAM across multiple computers.

Spark provides high-level APIs in Java, Scala, Python, and R. Spark code can be written in any of these four languages. It also provides a shell in Scala and Python.

 - Resilient Distributed Dataset (RDD)
 - Directed Acyclic Graph (DAG)



 There are two ways to create RDDs − parallelizing an existing collection in your driver program, or by referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, etc.



 update-alternatives: using /usr/share/scala-2.11/bin/scala to provide /usr/bin/scala (scala) in auto mode

****
Spark has three data representations viz RDD, Dataframe, Dataset

Using PySpark
-------------
https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/

Using Scala
-----------
https://www.analyticsvidhya.com/blog/2017/01/scala/

./spark-shell --driver-java-options=--illegal-access=warn --packages com.databricks:spark-csv_2.10:1.3.0

DataFrame:
./bin/spark-shell --packages com.databricks:spark-csv_2.10:1.3.0



./bin/spark-submit --class SimpleApp --master spark://localhost:7077 ~/IdeaProjects/SparkApp/target/scala-2.11/sparkapp_2.11-0.1.jar


bin/spark-submit --master local[*] --class Main --files /home/user/Work/Bitbucket/IOT/application.conf --driver-java-options -Dconfig.file=/home/user/Work/Bitbucket/IOT/application.conf ~/Work/Bitbucket/IOT/target/scala-2.11/KafkaStreamProcessor-assembly-0.1.jar


extraClassPath

bin/spark-submit --master spark://0.0.0.0:7077 --class root.Main --deploy-mode cluster --files /home/user/Work/Bitbucket/IOT/application.conf --conf spark.driver.extraJavaOptions=-Dconfig.file=/home/user/Work/Bitbucket/IOT/application.conf --conf spark.executor.extraJavaOptions=-Dconfig.file=/home/user/Work/Bitbucket/IOT/application.conf ~/Work/Bitbucket/IOT/target/scala-2.11/KafkaStreamProcessor-assembly-0.1.jar


Config
-------
http://florentfo.rest/2019/01/07/configuring-spark-applications-with-typesafe-config.html
https://stackoverflow.com/questions/53290715/submit-an-application-property-file-with-spark-typesafe-config
